{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pandas Walkthrough\n",
        "\n",
        "## Table of Contents\n",
        "1. [Setting Up and Importing Libraries](#1-setting-up-and-importing-libraries)\n",
        "2. [Creating Dummy Data](#2-creating-dummy-data)\n",
        "3. [Understanding DataFrames](#3-understanding-dataframes)\n",
        "4. [Loading and Saving Data](#4-loading-and-saving-data)\n",
        "5. [Basic Data Exploration](#5-basic-data-exploration)\n",
        "6. [Data Selection and Filtering](#6-data-selection-and-filtering)\n",
        "7. [Basic Data Processing](#7-basic-data-processing)\n",
        "8. [Handling Missing Data](#8-handling-missing-data)\n",
        "9. [Grouping and Aggregation](#9-grouping-and-aggregation)\n",
        "10. [Summary](#10-summary)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63ed9d7a",
      "metadata": {},
      "source": [
        "# Introduction to Pandas\n",
        "\n",
        "Pandas is one of the most widely used Python libraries for **data analysis** and **machine learning workflows**.  \n",
        "It provides fast, flexible, and expressive data structures that make working with structured data both easy and powerful.\n",
        "\n",
        "\n",
        "## Why Pandas?\n",
        "\n",
        "- **Data Cleaning**  \n",
        "  Handle missing values, remove duplicates, and correct inconsistencies in raw datasets.\n",
        "\n",
        "- **Data Transformation**  \n",
        "  Convert, reshape, merge, and filter datasets into the format required for analysis.\n",
        "\n",
        "- **Exploratory Data Analysis (EDA)**  \n",
        "  Summarise datasets using descriptive statistics, and quickly generate insights.\n",
        "\n",
        "- **Integration with Machine Learning**  \n",
        "  Prepare and feed clean, well-structured data into machine learning libraries such as Scikit-Learn.\n",
        "\n",
        "\n",
        "## Key Data Structures\n",
        "\n",
        "1. **Series**  \n",
        "   - A one-dimensional labelled array.  \n",
        "   - Useful for representing a single column or a list with labels (like an Excel column).\n",
        "\n",
        "2. **DataFrame**  \n",
        "   - A two-dimensional table of rows and columns.  \n",
        "   - The most commonly used structure in Pandas.  \n",
        "   - Similar to a spreadsheet or SQL table.\n",
        "\n",
        "\n",
        "## Example Use Cases in Data Analysis\n",
        "\n",
        "- Importing data from **CSV, Excel, SQL databases, or JSON** files.  \n",
        "- Cleaning messy datasets by filling missing values or correcting data types.  \n",
        "- Filtering large datasets to focus on relevant records.  \n",
        "- Grouping and aggregating values to calculate totals, averages, and other statistics.  \n",
        "- Creating quick visualisations of trends and distributions.\n",
        "\n",
        "\n",
        "## Example Use Cases in Machine Learning\n",
        "\n",
        "- **Preprocessing Data**: Encoding categorical variables, scaling numerical features, and handling null values.  \n",
        "- **Feature Engineering**: Creating new features that improve model performance.  \n",
        "- **Splitting Data**: Separating training and testing datasets to evaluate models properly.  \n",
        "- **Pipeline Support**: Feeding clean DataFrames directly into machine learning libraries.\n",
        "\n",
        "\n",
        "In short, Pandas acts as the **bridge between raw data and useful insights**, making it a core skill for anyone working in data analysis or machine learning.\n",
        "\n",
        "\n",
        "## 1. Setting Up and Importing Libraries\n",
        "\n",
        "First, let's import the essential libraries we'll need for this tutorial. Pandas is the main library for data manipulation, whilst NumPy helps with numerical operations and random data generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully!\n",
            "Pandas version: 2.0.3\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Set random seed for reproducible results\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "**What's happening here?**\n",
        "- `pandas` (imported as `pd`) is our main data manipulation library\n",
        "- `numpy` (imported as `np`) provides mathematical functions and random number generation\n",
        "- `random` and `datetime` help us create realistic dummy data\n",
        "- Setting a seed ensures we get the same \"random\" data each time we run the code\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Creating Dummy Data\n",
        "\n",
        "Let's create a realistic dataset about employees in a company. This will give us interesting data to work with throughout the tutorial.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dummy data created successfully!\n",
            "Dataset shape: (100, 9)\n"
          ]
        }
      ],
      "source": [
        "# Create dummy data for an employee database\n",
        "def create_employee_data(n_employees=100):\n",
        "    \"\"\"Create dummy employee data with realistic information\"\"\"\n",
        "    \n",
        "    # Define lists of possible values\n",
        "    departments = ['Sales', 'Marketing', 'IT', 'HR', 'Finance', 'Operations']\n",
        "    cities = ['London', 'Manchester', 'Birmingham', 'Glasgow', 'Bristol', 'Liverpool']\n",
        "    job_titles = ['Analyst', 'Manager', 'Specialist', 'Coordinator', 'Director', 'Assistant']\n",
        "    \n",
        "    # Generate random data\n",
        "    data = {\n",
        "        'employee_id': range(1001, 1001 + n_employees),\n",
        "        'first_name': [f\"Employee_{i}\" for i in range(1, n_employees + 1)],\n",
        "        'department': np.random.choice(departments, n_employees),\n",
        "        'job_title': np.random.choice(job_titles, n_employees),\n",
        "        'salary': np.random.normal(45000, 15000, n_employees).round(0).astype(int),\n",
        "        'years_experience': np.random.randint(0, 25, n_employees),\n",
        "        'city': np.random.choice(cities, n_employees),\n",
        "        'performance_score': np.random.uniform(1.0, 5.0, n_employees).round(2),\n",
        "        'start_date': [\n",
        "            datetime(2020, 1, 1) + timedelta(days=np.random.randint(0, 1460))\n",
        "            for _ in range(n_employees)\n",
        "        ]\n",
        "    }\n",
        "    \n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Create the dataset\n",
        "df = create_employee_data(100)\n",
        "\n",
        "# Add some missing values to make it more realistic\n",
        "df.loc[df.sample(5).index, 'performance_score'] = np.nan\n",
        "df.loc[df.sample(3).index, 'salary'] = np.nan\n",
        "\n",
        "print(\"Dummy data created successfully!\")\n",
        "print(f\"Dataset shape: {df.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "**What's happening here?**\n",
        "- We've created a function that generates realistic employee data\n",
        "- The dataset includes various data types: integers, floats, strings, and dates\n",
        "- We intentionally added some missing values (NaN) to demonstrate how to handle them later\n",
        "- `df.shape` tells us we have 100 rows and 9 columns\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Understanding DataFrames\n",
        "\n",
        "A DataFrame is like a spreadsheet or table with rows and columns. Let's explore the basic properties of our DataFrame.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== BASIC DATAFRAME INFORMATION ===\n",
            "Shape (rows, columns): (100, 9)\n",
            "Total number of elements: 900\n",
            "Memory usage: 29901 bytes\n",
            "\n",
            "=== COLUMN INFORMATION ===\n",
            "Column names:\n",
            "['employee_id', 'first_name', 'department', 'job_title', 'salary', 'years_experience', 'city', 'performance_score', 'start_date']\n",
            "\n",
            "Data types of each column:\n",
            "employee_id                   int64\n",
            "first_name                   object\n",
            "department                   object\n",
            "job_title                    object\n",
            "salary                      float64\n",
            "years_experience              int32\n",
            "city                         object\n",
            "performance_score           float64\n",
            "start_date           datetime64[ns]\n",
            "dtype: object\n",
            "\n",
            "=== FIRST 5 ROWS (HEAD) ===\n",
            "   employee_id  first_name department job_title   salary  years_experience  \\\n",
            "0         1001  Employee_1         HR  Director  49520.0                 9   \n",
            "1         1002  Employee_2    Finance   Analyst  23122.0                21   \n",
            "2         1003  Employee_3         IT   Analyst  35059.0                 2   \n",
            "3         1004  Employee_4    Finance   Analyst  42803.0                 7   \n",
            "4         1005  Employee_5    Finance   Analyst  32310.0                13   \n",
            "\n",
            "         city  performance_score start_date  \n",
            "0     Bristol               2.69 2023-09-15  \n",
            "1     Bristol               3.27 2022-01-05  \n",
            "2   Liverpool               3.30 2022-02-22  \n",
            "3     Bristol               3.93 2022-08-25  \n",
            "4  Birmingham               1.51 2020-12-23  \n",
            "\n",
            "=== LAST 5 ROWS (TAIL) ===\n",
            "    employee_id    first_name department    job_title   salary  \\\n",
            "95         1096   Employee_96         HR   Specialist  37871.0   \n",
            "96         1097   Employee_97         HR      Manager  35859.0   \n",
            "97         1098   Employee_98    Finance  Coordinator  32797.0   \n",
            "98         1099   Employee_99      Sales   Specialist  10229.0   \n",
            "99         1100  Employee_100    Finance    Assistant  40980.0   \n",
            "\n",
            "    years_experience        city  performance_score start_date  \n",
            "95                24     Bristol               4.98 2022-02-09  \n",
            "96                 7     Bristol               3.79 2023-08-22  \n",
            "97                12      London               2.54 2020-09-24  \n",
            "98                20     Glasgow               3.95 2022-12-13  \n",
            "99                 0  Manchester               4.66 2020-08-23  \n"
          ]
        }
      ],
      "source": [
        "# Basic information about our DataFrame\n",
        "print(\"=== BASIC DATAFRAME INFORMATION ===\")\n",
        "print(f\"Shape (rows, columns): {df.shape}\")\n",
        "print(f\"Total number of elements: {df.size}\")\n",
        "print(f\"Memory usage: {df.memory_usage(deep=True).sum()} bytes\")\n",
        "\n",
        "print(\"\\n=== COLUMN INFORMATION ===\")\n",
        "print(\"Column names:\")\n",
        "print(df.columns.tolist())\n",
        "\n",
        "print(\"\\nData types of each column:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "print(\"\\n=== FIRST 5 ROWS (HEAD) ===\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\n=== LAST 5 ROWS (TAIL) ===\")\n",
        "print(df.tail())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "**Key concepts:**\n",
        "- **Shape**: Shows (number of rows, number of columns)\n",
        "- **Columns**: The names of your data fields\n",
        "- **Data types**: Pandas automatically detects whether data is text (object), numbers (int64, float64), or dates (datetime64)\n",
        "- **Head/Tail**: Quick ways to peek at your data\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Loading and Saving Data\n",
        "\n",
        "Let's save our dummy data to a CSV file and then load it back. This is how you'd typically work with real data files.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data saved to employee_data.csv\n",
            "Data loaded from employee_data.csv\n",
            "Loaded data shape: (100, 9)\n",
            "Data identical after loading: False\n",
            "Converted start_date to datetime format\n",
            "\n",
            "=== COMMON CSV LOADING OPTIONS ===\n",
            "# Load only first 10 rows:\n",
            "df_sample = pd.read_csv('employee_data.csv', nrows=10)\n",
            "# Load specific columns only:\n",
            "df_subset = pd.read_csv('employee_data.csv', usecols=['first_name', 'department', 'salary'])\n",
            "# Skip header row:\n",
            "df_no_header = pd.read_csv('employee_data.csv', skiprows=1, names=custom_column_names)\n"
          ]
        }
      ],
      "source": [
        "# Save DataFrame to CSV\n",
        "csv_filename = 'employee_data.csv'\n",
        "df.to_csv(csv_filename, index=False)\n",
        "print(f\"Data saved to {csv_filename}\")\n",
        "\n",
        "# Load data from CSV\n",
        "df_loaded = pd.read_csv(csv_filename)\n",
        "print(f\"Data loaded from {csv_filename}\")\n",
        "print(f\"Loaded data shape: {df_loaded.shape}\")\n",
        "\n",
        "# Check if the data is identical\n",
        "print(f\"Data identical after loading: {df.equals(df_loaded)}\")\n",
        "\n",
        "# Note: Dates might load as strings, so let's convert them properly\n",
        "df_loaded['start_date'] = pd.to_datetime(df_loaded['start_date'])\n",
        "print(\"Converted start_date to datetime format\")\n",
        "\n",
        "# Common CSV loading options\n",
        "print(\"\\n=== COMMON CSV LOADING OPTIONS ===\")\n",
        "print(\"# Load only first 10 rows:\")\n",
        "print(\"df_sample = pd.read_csv('employee_data.csv', nrows=10)\")\n",
        "\n",
        "print(\"# Load specific columns only:\")\n",
        "print(\"df_subset = pd.read_csv('employee_data.csv', usecols=['first_name', 'department', 'salary'])\")\n",
        "\n",
        "print(\"# Skip header row:\")\n",
        "print(\"df_no_header = pd.read_csv('employee_data.csv', skiprows=1, names=custom_column_names)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "**Key points about CSV operations:**\n",
        "- `index=False` prevents pandas from saving row numbers as a column\n",
        "- Always check your data after loading to ensure it's correct\n",
        "- Dates often need manual conversion using `pd.to_datetime()`\n",
        "- You can load partial data using parameters like `nrows` or `usecols`\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Basic Data Exploration\n",
        "\n",
        "Now let's explore our data to understand what we're working with. This is always the first step in any data analysis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== COMPREHENSIVE DATA OVERVIEW ===\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 9 columns):\n",
            " #   Column             Non-Null Count  Dtype         \n",
            "---  ------             --------------  -----         \n",
            " 0   employee_id        100 non-null    int64         \n",
            " 1   first_name         100 non-null    object        \n",
            " 2   department         100 non-null    object        \n",
            " 3   job_title          100 non-null    object        \n",
            " 4   salary             97 non-null     float64       \n",
            " 5   years_experience   100 non-null    int32         \n",
            " 6   city               100 non-null    object        \n",
            " 7   performance_score  95 non-null     float64       \n",
            " 8   start_date         100 non-null    datetime64[ns]\n",
            "dtypes: datetime64[ns](1), float64(2), int32(1), int64(1), object(4)\n",
            "memory usage: 6.8+ KB\n",
            "None\n",
            "\n",
            "=== STATISTICAL SUMMARY ===\n",
            "       employee_id        salary  years_experience  performance_score  \\\n",
            "count   100.000000     97.000000        100.000000          95.000000   \n",
            "mean   1050.500000  46148.907216         12.670000           3.099684   \n",
            "min    1001.000000  10229.000000          0.000000           1.170000   \n",
            "25%    1025.750000  35059.000000          7.000000           2.095000   \n",
            "50%    1050.500000  45889.000000         13.000000           3.110000   \n",
            "75%    1075.250000  55806.000000         20.000000           3.960000   \n",
            "max    1100.000000  90916.000000         24.000000           4.980000   \n",
            "std      29.011492  14865.586167          7.513026           1.116227   \n",
            "\n",
            "                start_date  \n",
            "count                  100  \n",
            "mean   2021-12-23 00:43:12  \n",
            "min    2020-01-01 00:00:00  \n",
            "25%    2021-03-04 18:00:00  \n",
            "50%    2022-01-07 00:00:00  \n",
            "75%    2022-10-31 18:00:00  \n",
            "max    2023-12-02 00:00:00  \n",
            "std                    NaN  \n",
            "\n",
            "=== MISSING VALUES CHECK ===\n",
            "Missing values per column:\n",
            "salary               3\n",
            "performance_score    5\n",
            "dtype: int64\n",
            "\n",
            "Total missing values: 8\n",
            "Percentage of missing data: 0.89%\n",
            "\n",
            "=== UNIQUE VALUES IN CATEGORICAL COLUMNS ===\n",
            "\n",
            "DEPARTMENT:\n",
            "  Unique values: 6\n",
            "  Values: ['HR', 'Finance', 'IT', 'Marketing', 'Operations', 'Sales']\n",
            "\n",
            "JOB_TITLE:\n",
            "  Unique values: 6\n",
            "  Values: ['Director', 'Analyst', 'Coordinator', 'Specialist', 'Manager', 'Assistant']\n",
            "\n",
            "CITY:\n",
            "  Unique values: 6\n",
            "  Values: ['Bristol', 'Liverpool', 'Birmingham', 'Glasgow', 'Manchester', 'London']\n",
            "\n",
            "=== VALUE COUNTS FOR DEPARTMENTS ===\n",
            "department\n",
            "HR            25\n",
            "Marketing     19\n",
            "Finance       17\n",
            "Operations    17\n",
            "IT            11\n",
            "Sales         11\n",
            "Name: count, dtype: int64\n",
            "\n",
            "=== BASIC STATISTICS FOR SALARY ===\n",
            "count       97.000000\n",
            "mean     46148.907216\n",
            "std      14865.586167\n",
            "min      10229.000000\n",
            "25%      35059.000000\n",
            "50%      45889.000000\n",
            "75%      55806.000000\n",
            "max      90916.000000\n",
            "Name: salary, dtype: float64\n",
            "Salary range: $10,229 to $90,916\n"
          ]
        }
      ],
      "source": [
        "print(\"=== COMPREHENSIVE DATA OVERVIEW ===\")\n",
        "print(df.info())\n",
        "\n",
        "print(\"\\n=== STATISTICAL SUMMARY ===\")\n",
        "print(df.describe())\n",
        "\n",
        "print(\"\\n=== MISSING VALUES CHECK ===\")\n",
        "missing_data = df.isnull().sum()\n",
        "print(\"Missing values per column:\")\n",
        "print(missing_data[missing_data > 0])\n",
        "\n",
        "print(f\"\\nTotal missing values: {df.isnull().sum().sum()}\")\n",
        "print(f\"Percentage of missing data: {(df.isnull().sum().sum() / df.size) * 100:.2f}%\")\n",
        "\n",
        "print(\"\\n=== UNIQUE VALUES IN CATEGORICAL COLUMNS ===\")\n",
        "categorical_columns = ['department', 'job_title', 'city']\n",
        "for col in categorical_columns:\n",
        "    print(f\"\\n{col.upper()}:\")\n",
        "    print(f\"  Unique values: {df[col].nunique()}\")\n",
        "    print(f\"  Values: {df[col].unique().tolist()}\")\n",
        "\n",
        "print(\"\\n=== VALUE COUNTS FOR DEPARTMENTS ===\")\n",
        "print(df['department'].value_counts())\n",
        "\n",
        "print(\"\\n=== BASIC STATISTICS FOR SALARY ===\")\n",
        "salary_stats = df['salary'].describe()\n",
        "print(salary_stats)\n",
        "print(f\"Salary range: ${df['salary'].min():,.0f} to ${df['salary'].max():,.0f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "**What each function tells us:**\n",
        "- `info()`: Overall structure, data types, and memory usage\n",
        "- `describe()`: Statistical summary of numerical columns\n",
        "- `isnull().sum()`: Counts missing values in each column\n",
        "- `nunique()`: Number of unique values (useful for categorical data)\n",
        "- `value_counts()`: Frequency of each unique value\n",
        "- `unique()`: List of all unique values\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Data Selection and Filtering\n",
        "\n",
        "Learning how to select specific data is crucial. Let's explore different ways to slice and filter our DataFrame.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== SELECTING COLUMNS ===\n",
            "Single column (Series):\n",
            "Type: <class 'pandas.core.series.Series'>\n",
            "0         HR\n",
            "1    Finance\n",
            "2         IT\n",
            "3    Finance\n",
            "4    Finance\n",
            "Name: department, dtype: object\n",
            "\n",
            "Single column (DataFrame): Type: <class 'pandas.core.frame.DataFrame'>\n",
            "\n",
            "Multiple columns shape: (100, 3)\n",
            "   first_name department   salary\n",
            "0  Employee_1         HR  49520.0\n",
            "1  Employee_2    Finance  23122.0\n",
            "2  Employee_3         IT  35059.0\n",
            "3  Employee_4    Finance  42803.0\n",
            "4  Employee_5    Finance  32310.0\n",
            "\n",
            "=== SELECTING ROWS ===\n",
            "First 5 rows using iloc:\n",
            "   employee_id  first_name department job_title   salary  years_experience  \\\n",
            "0         1001  Employee_1         HR  Director  49520.0                 9   \n",
            "1         1002  Employee_2    Finance   Analyst  23122.0                21   \n",
            "2         1003  Employee_3         IT   Analyst  35059.0                 2   \n",
            "3         1004  Employee_4    Finance   Analyst  42803.0                 7   \n",
            "4         1005  Employee_5    Finance   Analyst  32310.0                13   \n",
            "\n",
            "         city  performance_score start_date  \n",
            "0     Bristol               2.69 2023-09-15  \n",
            "1     Bristol               3.27 2022-01-05  \n",
            "2   Liverpool               3.30 2022-02-22  \n",
            "3     Bristol               3.93 2022-08-25  \n",
            "4  Birmingham               1.51 2020-12-23  \n",
            "\n",
            "Rows 10-15 using iloc:\n",
            "    employee_id   first_name  department   job_title   salary  \\\n",
            "10         1011  Employee_11          HR  Specialist  63555.0   \n",
            "11         1012  Employee_12          IT     Analyst  23603.0   \n",
            "12         1013  Employee_13  Operations  Specialist  50070.0   \n",
            "13         1014  Employee_14     Finance    Director  74619.0   \n",
            "14         1015  Employee_15   Marketing     Manager  74476.0   \n",
            "\n",
            "    years_experience       city  performance_score start_date  \n",
            "10                 1     London               3.72 2023-02-19  \n",
            "11                 9    Bristol               3.96 2023-10-03  \n",
            "12                 1  Liverpool               1.95 2021-11-06  \n",
            "13                16    Glasgow               2.51 2020-03-30  \n",
            "14                 7    Bristol               3.14 2021-11-28  \n",
            "\n",
            "Specific rows and columns using iloc:\n",
            "   first_name department job_title\n",
            "0  Employee_1         HR  Director\n",
            "1  Employee_2    Finance   Analyst\n",
            "2  Employee_3         IT   Analyst\n",
            "3  Employee_4    Finance   Analyst\n",
            "4  Employee_5    Finance   Analyst\n",
            "\n",
            "=== FILTERING DATA (BOOLEAN INDEXING) ===\n",
            "Employees earning more than $60,000: 17\n",
            "     first_name department   salary\n",
            "6    Employee_7         IT  61301.0\n",
            "7    Employee_8         IT  60075.0\n",
            "10  Employee_11         HR  63555.0\n",
            "13  Employee_14    Finance  74619.0\n",
            "14  Employee_15  Marketing  74476.0\n",
            "\n",
            "IT employees earning more than $50,000: 5\n",
            "\n",
            "Sales or Marketing employees: 30\n",
            "\n",
            "Employees in London or Birmingham: 26\n",
            "\n",
            "Employees with 'Manager' in job title: 10\n",
            "\n",
            "=== QUERY METHOD (ALTERNATIVE FILTERING) ===\n",
            "Experienced analysts: 21\n",
            "High-performing London employees: 2\n"
          ]
        }
      ],
      "source": [
        "print(\"=== SELECTING COLUMNS ===\")\n",
        "# Select single column (returns a Series)\n",
        "department_series = df['department']\n",
        "print(\"Single column (Series):\")\n",
        "print(f\"Type: {type(department_series)}\")\n",
        "print(department_series.head())\n",
        "\n",
        "# Select single column (returns a DataFrame)\n",
        "department_df = df[['department']]\n",
        "print(f\"\\nSingle column (DataFrame): Type: {type(department_df)}\")\n",
        "\n",
        "# Select multiple columns\n",
        "selected_columns = df[['first_name', 'department', 'salary']]\n",
        "print(f\"\\nMultiple columns shape: {selected_columns.shape}\")\n",
        "print(selected_columns.head())\n",
        "\n",
        "print(\"\\n=== SELECTING ROWS ===\")\n",
        "# Select by position using iloc\n",
        "print(\"First 5 rows using iloc:\")\n",
        "print(df.iloc[:5])\n",
        "\n",
        "print(\"\\nRows 10-15 using iloc:\")\n",
        "print(df.iloc[10:15])\n",
        "\n",
        "print(\"\\nSpecific rows and columns using iloc:\")\n",
        "print(df.iloc[0:5, 1:4])  # First 5 rows, columns 1-3\n",
        "\n",
        "print(\"\\n=== FILTERING DATA (BOOLEAN INDEXING) ===\")\n",
        "# Simple filter\n",
        "high_earners = df[df['salary'] > 60000]\n",
        "print(f\"Employees earning more than $60,000: {len(high_earners)}\")\n",
        "print(high_earners[['first_name', 'department', 'salary']].head())\n",
        "\n",
        "# Multiple conditions with AND (&)\n",
        "it_high_earners = df[(df['department'] == 'IT') & (df['salary'] > 50000)]\n",
        "print(f\"\\nIT employees earning more than $50,000: {len(it_high_earners)}\")\n",
        "\n",
        "# Multiple conditions with OR (|)\n",
        "sales_or_marketing = df[(df['department'] == 'Sales') | (df['department'] == 'Marketing')]\n",
        "print(f\"\\nSales or Marketing employees: {len(sales_or_marketing)}\")\n",
        "\n",
        "# Using isin() for multiple values\n",
        "london_birmingham = df[df['city'].isin(['London', 'Birmingham'])]\n",
        "print(f\"\\nEmployees in London or Birmingham: {len(london_birmingham)}\")\n",
        "\n",
        "# String filtering\n",
        "managers = df[df['job_title'].str.contains('Manager', na=False)]\n",
        "print(f\"\\nEmployees with 'Manager' in job title: {len(managers)}\")\n",
        "\n",
        "print(\"\\n=== QUERY METHOD (ALTERNATIVE FILTERING) ===\")\n",
        "# Using query method (often more readable)\n",
        "experienced_analysts = df.query(\"job_title == 'Analyst' and years_experience > 5\")\n",
        "print(f\"Experienced analysts: {len(experienced_analysts)}\")\n",
        "\n",
        "# Query with string values\n",
        "london_high_performers = df.query(\"city == 'London' and performance_score > 4.0\")\n",
        "print(f\"High-performing London employees: {len(london_high_performers)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "**Key selection methods:**\n",
        "- **Single brackets `df['col']`**: Returns a Series\n",
        "- **Double brackets `df[['col']]`**: Returns a DataFrame\n",
        "- **`iloc`**: Integer-based location selection\n",
        "- **Boolean indexing**: Filter using conditions\n",
        "- **`&` and `|`**: AND and OR for combining conditions\n",
        "- **`isin()`**: Check if values are in a list\n",
        "- **`str.contains()`**: String pattern matching\n",
        "- **`query()`**: Alternative syntax for filtering\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Basic Data Processing\n",
        "\n",
        "Let's learn how to transform and modify our data. This includes creating new columns, modifying existing ones, and basic calculations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== CREATING NEW COLUMNS ===\n",
            "Salary bands created:\n",
            "salary_band\n",
            "Medium     48\n",
            "High       25\n",
            "Low        24\n",
            "Unknown     3\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Senior vs Junior employees:\n",
            "senior_employee\n",
            "Senior    63\n",
            "Junior    37\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Performance categories:\n",
            "performance_category\n",
            "Needs Improvement    46\n",
            "Good Performer       27\n",
            "High Performer       22\n",
            "Not Rated             5\n",
            "Name: count, dtype: int64\n",
            "\n",
            "=== MODIFYING EXISTING COLUMNS ===\n",
            "Salary in thousands ($k):\n",
            "   first_name   salary  salary_k\n",
            "0  Employee_1  49520.0      49.5\n",
            "1  Employee_2  23122.0      23.1\n",
            "2  Employee_3  35059.0      35.1\n",
            "3  Employee_4  42803.0      42.8\n",
            "4  Employee_5  32310.0      32.3\n",
            "\n",
            "String modifications:\n",
            "   first_name initials department department_upper\n",
            "0  Employee_1       E.         HR               HR\n",
            "1  Employee_2       E.    Finance          FINANCE\n",
            "2  Employee_3       E.         IT               IT\n",
            "3  Employee_4       E.    Finance          FINANCE\n",
            "4  Employee_5       E.    Finance          FINANCE\n",
            "\n",
            "=== SORTING DATA ===\n",
            "Top 5 earners:\n",
            "     first_name  department   salary\n",
            "71  Employee_72  Operations  90916.0\n",
            "84  Employee_85  Operations  86759.0\n",
            "13  Employee_14     Finance  74619.0\n",
            "14  Employee_15   Marketing  74476.0\n",
            "57  Employee_58          HR  71829.0\n",
            "\n",
            "Sorted by department (A-Z), then salary (high to low):\n",
            "      first_name department   salary\n",
            "13   Employee_14    Finance  74619.0\n",
            "55   Employee_56    Finance  60979.0\n",
            "88   Employee_89    Finance  54105.0\n",
            "25   Employee_26    Finance  53152.0\n",
            "42   Employee_43    Finance  48085.0\n",
            "75   Employee_76    Finance  45889.0\n",
            "9    Employee_10    Finance  43738.0\n",
            "20   Employee_21    Finance  42941.0\n",
            "3     Employee_4    Finance  42803.0\n",
            "99  Employee_100    Finance  40980.0\n",
            "\n",
            "=== BASIC CALCULATIONS AND AGGREGATIONS ===\n",
            "Average salary: $46,149\n",
            "Median salary: $45,889\n",
            "Salary standard deviation: $14,866\n",
            "\n",
            "Average salary by department:\n",
            "  Operations: $52,299\n",
            "  HR: $46,519\n",
            "  Sales: $46,047\n",
            "  Marketing: $44,675\n",
            "  IT: $43,763\n",
            "  Finance: $42,544\n",
            "\n",
            "=== RENAMING COLUMNS ===\n",
            "Columns after renaming:\n",
            "['employee_id', 'employee_name', 'department', 'job_title', 'salary']\n"
          ]
        }
      ],
      "source": [
        "print(\"=== CREATING NEW COLUMNS ===\")\n",
        "# Create a new column based on existing data\n",
        "df['salary_band'] = df['salary'].apply(lambda x: 'High' if x > 55000 \n",
        "                                      else 'Medium' if x > 35000 \n",
        "                                      else 'Low' if pd.notna(x) else 'Unknown')\n",
        "\n",
        "print(\"Salary bands created:\")\n",
        "print(df['salary_band'].value_counts())\n",
        "\n",
        "# Create column using numpy where\n",
        "df['senior_employee'] = np.where(df['years_experience'] >= 10, 'Senior', 'Junior')\n",
        "print(f\"\\nSenior vs Junior employees:\")\n",
        "print(df['senior_employee'].value_counts())\n",
        "\n",
        "# Create a column based on multiple conditions\n",
        "def categorise_employee(row):\n",
        "    if pd.isna(row['performance_score']):\n",
        "        return 'Not Rated'\n",
        "    elif row['performance_score'] >= 4.0:\n",
        "        return 'High Performer'\n",
        "    elif row['performance_score'] >= 3.0:\n",
        "        return 'Good Performer'\n",
        "    else:\n",
        "        return 'Needs Improvement'\n",
        "\n",
        "df['performance_category'] = df.apply(categorise_employee, axis=1)\n",
        "print(f\"\\nPerformance categories:\")\n",
        "print(df['performance_category'].value_counts())\n",
        "\n",
        "print(\"\\n=== MODIFYING EXISTING COLUMNS ===\")\n",
        "# Convert salary to thousands for easier reading\n",
        "df['salary_k'] = (df['salary'] / 1000).round(1)\n",
        "print(\"Salary in thousands ($k):\")\n",
        "print(df[['first_name', 'salary', 'salary_k']].head())\n",
        "\n",
        "# Modify string columns\n",
        "df['department_upper'] = df['department'].str.upper()\n",
        "df['initials'] = df['first_name'].str[0] + '.'\n",
        "print(f\"\\nString modifications:\")\n",
        "print(df[['first_name', 'initials', 'department', 'department_upper']].head())\n",
        "\n",
        "print(\"\\n=== SORTING DATA ===\")\n",
        "# Sort by single column\n",
        "salary_sorted = df.sort_values('salary', ascending=False)\n",
        "print(\"Top 5 earners:\")\n",
        "print(salary_sorted[['first_name', 'department', 'salary']].head())\n",
        "\n",
        "# Sort by multiple columns\n",
        "multi_sorted = df.sort_values(['department', 'salary'], ascending=[True, False])\n",
        "print(\"\\nSorted by department (A-Z), then salary (high to low):\")\n",
        "print(multi_sorted[['first_name', 'department', 'salary']].head(10))\n",
        "\n",
        "print(\"\\n=== BASIC CALCULATIONS AND AGGREGATIONS ===\")\n",
        "# Basic statistics\n",
        "print(f\"Average salary: ${df['salary'].mean():,.0f}\")\n",
        "print(f\"Median salary: ${df['salary'].median():,.0f}\")\n",
        "print(f\"Salary standard deviation: ${df['salary'].std():,.0f}\")\n",
        "\n",
        "# Calculations by group\n",
        "dept_avg_salary = df.groupby('department')['salary'].mean().sort_values(ascending=False)\n",
        "print(f\"\\nAverage salary by department:\")\n",
        "for dept, avg_sal in dept_avg_salary.items():\n",
        "    print(f\"  {dept}: ${avg_sal:,.0f}\")\n",
        "\n",
        "print(\"\\n=== RENAMING COLUMNS ===\")\n",
        "# Rename specific columns\n",
        "df_renamed = df.rename(columns={\n",
        "    'first_name': 'employee_name',\n",
        "    'years_experience': 'experience_years'\n",
        "})\n",
        "print(\"Columns after renaming:\")\n",
        "print(df_renamed.columns.tolist()[:5])  # Show first 5 column names\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "**Key data processing techniques:**\n",
        "- **`apply()` with lambda**: Apply functions to columns\n",
        "- **`np.where()`**: Conditional value assignment\n",
        "- **Custom functions with `apply()`**: More complex logic\n",
        "- **String operations**: `.str` accessor for text manipulation\n",
        "- **`sort_values()`**: Sorting by one or multiple columns\n",
        "- **`groupby()`**: Group data for calculations\n",
        "- **`rename()`**: Change column names\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Handling Missing Data\n",
        "\n",
        "Missing data is common in real datasets. Let's learn different strategies to handle it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== IDENTIFYING MISSING DATA ===\n",
            "Missing values per column:\n",
            "salary               3\n",
            "performance_score    5\n",
            "salary_k             3\n",
            "dtype: int64\n",
            "\n",
            "Rows with any missing data: 8\n",
            "Rows with all data complete: 92\n",
            "\n",
            "Rows with missing values:\n",
            "    employee_id   first_name   salary  performance_score\n",
            "6          1007   Employee_7  61301.0                NaN\n",
            "15         1016  Employee_16      NaN               2.99\n",
            "32         1033  Employee_33  58313.0                NaN\n",
            "35         1036  Employee_36  40951.0                NaN\n",
            "45         1046  Employee_46  56119.0                NaN\n",
            "54         1055  Employee_55  40718.0                NaN\n",
            "61         1062  Employee_62      NaN               3.83\n",
            "85         1086  Employee_86      NaN               2.29\n",
            "\n",
            "=== DIFFERENT STRATEGIES FOR HANDLING MISSING DATA ===\n",
            "Original rows: 100, After dropping rows with NaN: 92\n",
            "Original columns: 15, After dropping columns with NaN: 12\n",
            "Missing values after filling with constants: 3\n",
            "Filled missing salaries with median: $45,889\n",
            "Filled missing performance scores with mean: 3.10\n",
            "\n",
            "=== CHECKING OUR WORK ===\n",
            "Missing values after different strategies:\n",
            "Original: 11\n",
            "Dropped rows: 0\n",
            "Filled with constants: 3\n",
            "Filled with statistics: 3\n",
            "Forward/backward fill: 6\n",
            "\n",
            "Using statistically filled dataset. Missing values: 3\n"
          ]
        }
      ],
      "source": [
        "print(\"=== IDENTIFYING MISSING DATA ===\")\n",
        "# Check for missing values\n",
        "print(\"Missing values per column:\")\n",
        "missing_summary = df.isnull().sum()\n",
        "print(missing_summary[missing_summary > 0])\n",
        "\n",
        "# Visualise missing data patterns\n",
        "print(f\"\\nRows with any missing data: {df.isnull().any(axis=1).sum()}\")\n",
        "print(f\"Rows with all data complete: {df.dropna().shape[0]}\")\n",
        "\n",
        "# Show rows with missing data\n",
        "print(\"\\nRows with missing values:\")\n",
        "missing_rows = df[df.isnull().any(axis=1)]\n",
        "print(missing_rows[['employee_id', 'first_name', 'salary', 'performance_score']])\n",
        "\n",
        "print(\"\\n=== DIFFERENT STRATEGIES FOR HANDLING MISSING DATA ===\")\n",
        "\n",
        "# Strategy 1: Drop rows with missing data\n",
        "df_dropped_rows = df.dropna()\n",
        "print(f\"Original rows: {len(df)}, After dropping rows with NaN: {len(df_dropped_rows)}\")\n",
        "\n",
        "# Strategy 2: Drop columns with missing data\n",
        "df_dropped_cols = df.dropna(axis=1)\n",
        "print(f\"Original columns: {df.shape[1]}, After dropping columns with NaN: {df_dropped_cols.shape[1]}\")\n",
        "\n",
        "# Strategy 3: Fill missing values with a constant\n",
        "df_filled_constant = df.copy()\n",
        "df_filled_constant['salary'].fillna(0, inplace=True)\n",
        "df_filled_constant['performance_score'].fillna('Not Rated', inplace=True)\n",
        "print(f\"Missing values after filling with constants: {df_filled_constant.isnull().sum().sum()}\")\n",
        "\n",
        "# Strategy 4: Fill with statistical measures\n",
        "df_filled_stats = df.copy()\n",
        "# Fill salary with median\n",
        "median_salary = df['salary'].median()\n",
        "df_filled_stats['salary'].fillna(median_salary, inplace=True)\n",
        "print(f\"Filled missing salaries with median: ${median_salary:,.0f}\")\n",
        "\n",
        "# Fill performance score with mean\n",
        "mean_performance = df['performance_score'].mean()\n",
        "df_filled_stats['performance_score'].fillna(mean_performance, inplace=True)\n",
        "print(f\"Filled missing performance scores with mean: {mean_performance:.2f}\")\n",
        "\n",
        "# Strategy 5: Forward fill and backward fill\n",
        "df_ffill = df.copy()\n",
        "df_ffill['performance_score'].fillna(method='ffill', inplace=True)  # Forward fill\n",
        "df_ffill['performance_score'].fillna(method='bfill', inplace=True)  # Backward fill\n",
        "\n",
        "print(\"\\n=== CHECKING OUR WORK ===\")\n",
        "print(\"Missing values after different strategies:\")\n",
        "print(f\"Original: {df.isnull().sum().sum()}\")\n",
        "print(f\"Dropped rows: {df_dropped_rows.isnull().sum().sum()}\")\n",
        "print(f\"Filled with constants: {df_filled_constant.isnull().sum().sum()}\")\n",
        "print(f\"Filled with statistics: {df_filled_stats.isnull().sum().sum()}\")\n",
        "print(f\"Forward/backward fill: {df_ffill.isnull().sum().sum()}\")\n",
        "\n",
        "# Let's use the statistically filled version for the rest of our analysis\n",
        "df = df_filled_stats.copy()\n",
        "print(f\"\\nUsing statistically filled dataset. Missing values: {df.isnull().sum().sum()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "**Missing data strategies:**\n",
        "- **`dropna()`**: Remove rows or columns with missing values\n",
        "- **`fillna()`**: Replace missing values with specified values\n",
        "- **Statistical fills**: Use mean, median, or mode to fill gaps\n",
        "- **Forward/backward fill**: Use adjacent values to fill gaps\n",
        "- **Domain-specific fills**: Use business logic to determine appropriate values\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Grouping and Aggregation\n",
        "\n",
        "Grouping data allows us to calculate statistics for different categories. This is essential for data analysis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== BASIC GROUPING ===\n",
            "Employee count by department:\n",
            "department\n",
            "Finance       17\n",
            "HR            25\n",
            "IT            11\n",
            "Marketing     19\n",
            "Operations    17\n",
            "Sales         11\n",
            "dtype: int64\n",
            "\n",
            "Average salary by department:\n",
            "  Operations: $52,299\n",
            "  HR: $46,468\n",
            "  Sales: $46,047\n",
            "  Marketing: $44,675\n",
            "  IT: $43,763\n",
            "  Finance: $42,741\n",
            "\n",
            "=== MULTIPLE AGGREGATIONS ===\n",
            "Comprehensive salary statistics by department:\n",
            "            Count     Mean   Median      Min      Max  Std_Dev\n",
            "department                                                    \n",
            "Finance        17  42741.0  42941.0  15837.0  74619.0  14003.0\n",
            "HR             25  46468.0  45889.0  15954.0  71829.0  11355.0\n",
            "IT             11  43763.0  47260.0  23348.0  61301.0  14562.0\n",
            "Marketing      19  44675.0  44554.0  19342.0  74476.0  15915.0\n",
            "Operations     17  52299.0  49738.0  33561.0  90916.0  16104.0\n",
            "Sales          11  46047.0  52076.0  10229.0  66020.0  17907.0\n",
            "\n",
            "=== GROUPING BY MULTIPLE COLUMNS ===\n",
            "Average salary by department and city:\n",
            "department  city      \n",
            "Finance     Birmingham    34981.0\n",
            "            Bristol       42211.0\n",
            "            Glasgow       52452.0\n",
            "            Liverpool     43738.0\n",
            "            London        46888.0\n",
            "            Manchester    40373.0\n",
            "HR          Birmingham    54349.0\n",
            "            Bristol       38543.0\n",
            "            Glasgow       42976.0\n",
            "            Liverpool     48858.0\n",
            "            London        48498.0\n",
            "            Manchester    54026.0\n",
            "IT          Birmingham    60075.0\n",
            "            Bristol       34199.0\n",
            "            Liverpool     35059.0\n",
            "            London        52680.0\n",
            "            Manchester    48035.0\n",
            "Marketing   Birmingham    57242.0\n",
            "            Bristol       48708.0\n",
            "            Glasgow       48906.0\n",
            "            Liverpool     25172.0\n",
            "            London        39573.0\n",
            "            Manchester    42938.0\n",
            "Operations  Birmingham    86759.0\n",
            "            Bristol       52686.0\n",
            "            Glasgow       43649.0\n",
            "            Liverpool     46419.0\n",
            "            London        44764.0\n",
            "            Manchester    57433.0\n",
            "Sales       Birmingham    60007.0\n",
            "            Glasgow       43782.0\n",
            "            Liverpool     41944.0\n",
            "            Manchester    47902.0\n",
            "Name: salary, dtype: float64\n",
            "\n",
            "Salary table (departments vs cities):\n",
            "city        Birmingham  Bristol  Glasgow  Liverpool   London  Manchester\n",
            "department                                                              \n",
            "Finance        34981.0  42211.0  52452.0    43738.0  46888.0     40373.0\n",
            "HR             54349.0  38543.0  42976.0    48858.0  48498.0     54026.0\n",
            "IT             60075.0  34199.0      0.0    35059.0  52680.0     48035.0\n",
            "Marketing      57242.0  48708.0  48906.0    25172.0  39573.0     42938.0\n",
            "Operations     86759.0  52686.0  43649.0    46419.0  44764.0     57433.0\n",
            "Sales          60007.0      0.0  43782.0    41944.0      0.0     47902.0\n",
            "\n",
            "=== CUSTOM AGGREGATIONS ===\n",
            "Custom aggregations by department:\n",
            "              salary              performance_score                        \\\n",
            "                mean salary_range              mean high_performers_count   \n",
            "department                                                                  \n",
            "Finance     42740.94      58782.0              3.37                     6   \n",
            "HR          46468.28      55875.0              3.02                     5   \n",
            "IT          43763.27      37953.0              3.65                     2   \n",
            "Marketing   44674.79      55134.0              2.77                     4   \n",
            "Operations  52298.76      57355.0              3.32                     4   \n",
            "Sales       46046.55      55791.0              2.55                     1   \n",
            "\n",
            "           years_experience  \n",
            "                       mean  \n",
            "department                   \n",
            "Finance               13.06  \n",
            "HR                    13.44  \n",
            "IT                    11.73  \n",
            "Marketing             12.21  \n",
            "Operations            11.59  \n",
            "Sales                 13.73  \n",
            "\n",
            "=== FILTERING GROUPS ===\n",
            "Employees in departments with 15+ people: 78\n",
            "Large departments: ['HR', 'Finance', 'Marketing', 'Operations']\n",
            "\n",
            "=== PIVOT TABLES ===\n",
            "Pivot table: Average salary by department and seniority\n",
            "senior_employee   Junior   Senior\n",
            "department                       \n",
            "Finance          42915.0  42668.0\n",
            "HR               51266.0  43770.0\n",
            "IT               27640.0  52977.0\n",
            "Marketing        52379.0  39072.0\n",
            "Operations       44943.0  56311.0\n",
            "Sales            48963.0  43616.0\n",
            "\n",
            "Complex pivot table:\n",
            "            performance_score                         salary            \\\n",
            "salary_band              High   Low Medium Unknown      High       Low   \n",
            "department                                                               \n",
            "Finance                  3.48  2.59   3.73    3.83  67799.00  26730.00   \n",
            "HR                       2.86  2.29   3.27    2.64  62647.40  28304.00   \n",
            "IT                       3.80  3.64   3.57    0.00  59825.33  25166.67   \n",
            "Marketing                3.02  2.33   3.19    0.00  63133.50  29044.50   \n",
            "Operations               3.14  2.00   3.62    0.00  75457.25  33878.50   \n",
            "Sales                    2.79  3.03   1.66    0.00  60305.80  20774.00   \n",
            "\n",
            "                               \n",
            "salary_band    Medium Unknown  \n",
            "department                     \n",
            "Finance      45717.67   45889  \n",
            "HR           44785.33   45889  \n",
            "IT           45284.00       0  \n",
            "Marketing    47532.80       0  \n",
            "Operations   47226.64       0  \n",
            "Sales        47553.67       0  \n",
            "\n",
            "=== CROSS-TABULATION ===\n",
            "Cross-tabulation: Department vs Salary Band\n",
            "salary_band  High  Low  Medium  Unknown  All\n",
            "department                                  \n",
            "Finance         2    5       9        1   17\n",
            "HR              5    3      15        2   25\n",
            "IT              3    3       5        0   11\n",
            "Marketing       6    8       5        0   19\n",
            "Operations      4    2      11        0   17\n",
            "Sales           5    3       3        0   11\n",
            "All            25   24      48        3  100\n",
            "\n",
            "Percentage distribution within each department:\n",
            "salary_band  High   Low  Medium  Unknown\n",
            "department                              \n",
            "Finance      11.8  29.4    52.9      5.9\n",
            "HR           20.0  12.0    60.0      8.0\n",
            "IT           27.3  27.3    45.5      0.0\n",
            "Marketing    31.6  42.1    26.3      0.0\n",
            "Operations   23.5  11.8    64.7      0.0\n",
            "Sales        45.5  27.3    27.3      0.0\n"
          ]
        }
      ],
      "source": [
        "print(\"=== BASIC GROUPING ===\")\n",
        "# Group by single column\n",
        "dept_groups = df.groupby('department')\n",
        "\n",
        "# Basic statistics by group\n",
        "print(\"Employee count by department:\")\n",
        "print(dept_groups.size())\n",
        "\n",
        "print(f\"\\nAverage salary by department:\")\n",
        "dept_salary_avg = dept_groups['salary'].mean().sort_values(ascending=False)\n",
        "for dept, avg in dept_salary_avg.items():\n",
        "    print(f\"  {dept}: ${avg:,.0f}\")\n",
        "\n",
        "print(\"\\n=== MULTIPLE AGGREGATIONS ===\")\n",
        "# Multiple statistics at once\n",
        "dept_stats = dept_groups['salary'].agg(['count', 'mean', 'median', 'min', 'max', 'std'])\n",
        "dept_stats.columns = ['Count', 'Mean', 'Median', 'Min', 'Max', 'Std_Dev']\n",
        "dept_stats = dept_stats.round(0)\n",
        "print(\"Comprehensive salary statistics by department:\")\n",
        "print(dept_stats)\n",
        "\n",
        "print(\"\\n=== GROUPING BY MULTIPLE COLUMNS ===\")\n",
        "# Group by department and city\n",
        "dept_city_groups = df.groupby(['department', 'city'])\n",
        "dept_city_avg = dept_city_groups['salary'].mean().round(0)\n",
        "print(\"Average salary by department and city:\")\n",
        "print(dept_city_avg)\n",
        "\n",
        "# Convert to more readable format\n",
        "dept_city_table = dept_city_avg.unstack(fill_value=0)\n",
        "print(f\"\\nSalary table (departments vs cities):\")\n",
        "print(dept_city_table)\n",
        "\n",
        "print(\"\\n=== CUSTOM AGGREGATIONS ===\")\n",
        "# Define custom aggregation functions\n",
        "def salary_range(series):\n",
        "    return series.max() - series.min()\n",
        "\n",
        "def high_performers_count(series):\n",
        "    return (series > 4.0).sum()\n",
        "\n",
        "# Apply custom functions\n",
        "dept_custom = dept_groups.agg({\n",
        "    'salary': ['mean', salary_range],\n",
        "    'performance_score': ['mean', high_performers_count],\n",
        "    'years_experience': 'mean'\n",
        "}).round(2)\n",
        "\n",
        "print(\"Custom aggregations by department:\")\n",
        "print(dept_custom)\n",
        "\n",
        "print(\"\\n=== FILTERING GROUPS ===\")\n",
        "# Filter groups based on conditions\n",
        "large_departments = dept_groups.filter(lambda x: len(x) >= 15)\n",
        "print(f\"Employees in departments with 15+ people: {len(large_departments)}\")\n",
        "\n",
        "# Show which departments are large\n",
        "large_dept_names = large_departments['department'].unique()\n",
        "print(f\"Large departments: {large_dept_names.tolist()}\")\n",
        "\n",
        "print(\"\\n=== PIVOT TABLES ===\")\n",
        "# Create pivot table (like Excel pivot tables)\n",
        "pivot_performance = df.pivot_table(\n",
        "    values='salary',\n",
        "    index='department',\n",
        "    columns='senior_employee',\n",
        "    aggfunc='mean',\n",
        "    fill_value=0\n",
        ").round(0)\n",
        "\n",
        "print(\"Pivot table: Average salary by department and seniority\")\n",
        "print(pivot_performance)\n",
        "\n",
        "# More complex pivot table\n",
        "pivot_complex = df.pivot_table(\n",
        "    values=['salary', 'performance_score'],\n",
        "    index='department',\n",
        "    columns='salary_band',\n",
        "    aggfunc={'salary': 'mean', 'performance_score': 'mean'},\n",
        "    fill_value=0\n",
        ").round(2)\n",
        "\n",
        "print(f\"\\nComplex pivot table:\")\n",
        "print(pivot_complex)\n",
        "\n",
        "print(\"\\n=== CROSS-TABULATION ===\")\n",
        "# Cross-tabulation (frequency tables)\n",
        "crosstab = pd.crosstab(df['department'], df['salary_band'], margins=True)\n",
        "print(\"Cross-tabulation: Department vs Salary Band\")\n",
        "print(crosstab)\n",
        "\n",
        "# Percentage cross-tabulation\n",
        "crosstab_pct = pd.crosstab(df['department'], df['salary_band'], normalize='index') * 100\n",
        "crosstab_pct = crosstab_pct.round(1)\n",
        "print(f\"\\nPercentage distribution within each department:\")\n",
        "print(crosstab_pct)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "**Grouping and aggregation concepts:**\n",
        "- **`groupby()`**: Split data into groups based on column values\n",
        "- **`agg()`**: Apply multiple functions to grouped data\n",
        "- **Multiple grouping**: Group by several columns simultaneously\n",
        "- **Custom functions**: Write your own aggregation functions\n",
        "- **`filter()`**: Keep only groups that meet certain conditions\n",
        "- **`pivot_table()`**: Reshape data like Excel pivot tables\n",
        "- **`crosstab()`**: Create frequency tables\n",
        "\n",
        "---\n",
        "\n",
        "## 10. Summary\n",
        "\n",
        "Congratulations! You've learned the fundamentals of pandas for data manipulation and analysis. Let's summarise what we've covered:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== FINAL DATA SUMMARY ===\n",
            "Dataset shape: (100, 15)\n",
            "Columns: ['employee_id', 'first_name', 'department', 'job_title', 'salary', 'years_experience', 'city', 'performance_score', 'start_date', 'salary_band', 'senior_employee', 'performance_category', 'salary_k', 'department_upper', 'initials']\n",
            "Data types: {dtype('O'): 9, dtype('float64'): 3, dtype('int64'): 1, dtype('int32'): 1, dtype('<M8[ns]'): 1}\n",
            "Missing values: 3\n",
            "\n",
            "=== KEY INSIGHTS FROM OUR DATA ===\n",
            "Total employees: 100\n",
            "Departments: 6 (HR, Finance, IT, Marketing, Operations, Sales)\n",
            "Average salary: $46,141\n",
            "Salary range: $10,229 - $90,916\n",
            "Senior employees: 63\n",
            "High performers: 22\n",
            "\n",
            "=== WHAT YOU'VE LEARNED ===\n",
            " 1. Creating and understanding DataFrames\n",
            " 2. Loading and saving CSV files\n",
            " 3. Exploring data with info(), describe(), and value_counts()\n",
            " 4. Selecting columns and filtering rows\n",
            " 5. Creating new columns and modifying existing ones\n",
            " 6. Sorting and basic calculations\n",
            " 7. Handling missing data with various strategies\n",
            " 8. Grouping data and calculating aggregations\n",
            " 9. Creating pivot tables and cross-tabulations\n",
            "\n",
            "=== NEXT STEPS ===\n",
            "1. Learn data visualisation with matplotlib or seaborn\n",
            "2. Explore advanced pandas functions like merge() and join()\n",
            "3. Practice with real datasets from your domain\n",
            "4. Learn about time series analysis with pandas\n",
            "5. Explore statistical analysis and machine learning\n",
            "\n",
            "=== SAVE YOUR WORK ===\n",
            "Processed data saved as: processed_employee_data.csv\n",
            "\n",
            "Happy analysing! \n"
          ]
        }
      ],
      "source": [
        "print(\"=== FINAL DATA SUMMARY ===\")\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Columns: {list(df.columns)}\")\n",
        "print(f\"Data types: {df.dtypes.value_counts().to_dict()}\")\n",
        "print(f\"Missing values: {df.isnull().sum().sum()}\")\n",
        "\n",
        "print(\"\\n=== KEY INSIGHTS FROM OUR DATA ===\")\n",
        "print(f\"Total employees: {len(df)}\")\n",
        "print(f\"Departments: {df['department'].nunique()} ({', '.join(df['department'].unique())})\")\n",
        "print(f\"Average salary: ${df['salary'].mean():,.0f}\")\n",
        "print(f\"Salary range: ${df['salary'].min():,.0f} - ${df['salary'].max():,.0f}\")\n",
        "print(f\"Senior employees: {(df['senior_employee'] == 'Senior').sum()}\")\n",
        "print(f\"High performers: {(df['performance_category'] == 'High Performer').sum()}\")\n",
        "\n",
        "print(\"\\n=== WHAT YOU'VE LEARNED ===\")\n",
        "skills_learned = [\n",
        "    \"Creating and understanding DataFrames\",\n",
        "    \"Loading and saving CSV files\", \n",
        "    \"Exploring data with info(), describe(), and value_counts()\",\n",
        "    \"Selecting columns and filtering rows\",\n",
        "    \"Creating new columns and modifying existing ones\",\n",
        "    \"Sorting and basic calculations\",\n",
        "    \"Handling missing data with various strategies\",\n",
        "    \"Grouping data and calculating aggregations\",\n",
        "    \"Creating pivot tables and cross-tabulations\"\n",
        "]\n",
        "\n",
        "for i, skill in enumerate(skills_learned, 1):\n",
        "    print(f\"{i:2d}. {skill}\")\n",
        "\n",
        "print(\"\\n=== NEXT STEPS ===\")\n",
        "next_steps = [\n",
        "    \"Learn data visualisation with matplotlib or seaborn\",\n",
        "    \"Explore advanced pandas functions like merge() and join()\",\n",
        "    \"Practice with real datasets from your domain\",\n",
        "    \"Learn about time series analysis with pandas\",\n",
        "    \"Explore statistical analysis and machine learning\"\n",
        "]\n",
        "\n",
        "for i, step in enumerate(next_steps, 1):\n",
        "    print(f\"{i}. {step}\")\n",
        "\n",
        "print(\"\\n=== SAVE YOUR WORK ===\")\n",
        "# Save the processed dataset\n",
        "final_filename = 'processed_employee_data.csv'\n",
        "df.to_csv(final_filename, index=False)\n",
        "print(f\"Processed data saved as: {final_filename}\")\n",
        "\n",
        "print(\"\\nHappy analysing! \")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "**Essential pandas concepts you now understand:**\n",
        "- DataFrames are like spreadsheets with powerful manipulation capabilities\n",
        "- Always explore your data first using `info()`, `describe()`, and `head()`\n",
        "- Boolean indexing is your friend for filtering data\n",
        "- Missing data is normal - choose the right strategy for your situation\n",
        "- Groupby operations are powerful for calculating statistics by category\n",
        "- Pandas integrates well with other Python libraries for further analysis\n",
        "\n",
        "**Best practices to remember:**\n",
        "- Always check your data after loading or transforming it\n",
        "- Use meaningful variable names and add comments to your code\n",
        "- Save intermediate results when doing complex transformations\n",
        "- Start with simple operations before moving to complex ones\n",
        "- Practice with different datasets to reinforce your learning\n",
        "\n",
        "This tutorial provides a solid foundation for data analysis with pandas. Each concept builds upon the previous ones, so take your time to understand each section before moving on."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
