{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b74eaeb8",
   "metadata": {},
   "source": [
    "# üß© Feature Selection for Absolute Beginners\n",
    "\n",
    "**Learning Goal:** Learn how to choose the best features for your machine learning models to make them work better and faster.\n",
    "\n",
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "534ec3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb10be1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîç STEP 1: What are features?\n",
    "\n",
    "### Q1: When building a machine learning model, what information do we give it?\n",
    "\n",
    "**Answer:** We give it features - these are the input variables that describe our data.\n",
    "\n",
    "**Simple Example:**\n",
    "Think of predicting house prices. Our features might be:\n",
    "- Number of bedrooms\n",
    "- House size\n",
    "- Location\n",
    "- Age of house"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0068c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our house dataset:\n",
      "   bedrooms  size_sqft  age_years location   price\n",
      "0         2       1000         10     City  200000\n",
      "1         3       1500          5   Suburb  300000\n",
      "2         4       2000         15     City  400000\n",
      "3         2        900         20    Rural  150000\n",
      "4         5       2500          2     City  500000\n"
     ]
    }
   ],
   "source": [
    "# Create a simple house price dataset\n",
    "house_data = {\n",
    "    'bedrooms': [2, 3, 4, 2, 5],\n",
    "    'size_sqft': [1000, 1500, 2000, 900, 2500],\n",
    "    'age_years': [10, 5, 15, 20, 2],\n",
    "    'location': ['City', 'Suburb', 'City', 'Rural', 'City'],\n",
    "    'price': [200000, 300000, 400000, 150000, 500000]  # This is our target\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(house_data)\n",
    "print(\"Our house dataset:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a8992d",
   "metadata": {},
   "source": [
    "**Key Point:** Features are the characteristics we use to make predictions. The target (price) is what we want to predict.\n",
    "\n",
    "---\n",
    "\n",
    "## üîç STEP 2: Why feature selection matters\n",
    "\n",
    "### Q2: Why can't we just use all available features?\n",
    "\n",
    "**Answer:** Too many features can confuse the model and make it perform worse.\n",
    "\n",
    "**Problems with too many features:**\n",
    "1. **Overfitting** - Model memorizes instead of learning\n",
    "2. **Slow training** - More features = more computation\n",
    "3. **Noise** - Irrelevant features add confusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58a06580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset with useless features:\n",
      "   bedrooms  size_sqft  age_years location   price  house_id  random_number\n",
      "0         2       1000         10     City  200000         1             42\n",
      "1         3       1500          5   Suburb  300000         2             17\n",
      "2         4       2000         15     City  400000         3             88\n",
      "3         2        900         20    Rural  150000         4             33\n",
      "4         5       2500          2     City  500000         5             91\n",
      "\n",
      "Useful features: bedrooms, size_sqft, age_years, location\n",
      "Useless features: house_id, random_number\n"
     ]
    }
   ],
   "source": [
    "# Example: Adding useless features\n",
    "df['house_id'] = [1, 2, 3, 4, 5]  # Just an ID number\n",
    "df['random_number'] = [42, 17, 88, 33, 91]  # Random noise\n",
    "\n",
    "print(\"Dataset with useless features:\")\n",
    "print(df)\n",
    "\n",
    "# These features won't help predict price!\n",
    "print(\"\\nUseful features: bedrooms, size_sqft, age_years, location\")\n",
    "print(\"Useless features: house_id, random_number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a162cc50",
   "metadata": {},
   "source": [
    "**Simple Rule:** More features ‚â† Better model. We want the RIGHT features, not ALL features.\n",
    "\n",
    "---\n",
    "\n",
    "## üîç STEP 3: Manual Selection\n",
    "\n",
    "### Q3: How do we manually remove bad features?\n",
    "\n",
    "**Answer:** Use common sense and domain knowledge to remove obviously useless features.\n",
    "\n",
    "**What to remove:**\n",
    "1. **ID columns** (like customer_id, order_number)\n",
    "2. **Random data** (meaningless numbers)\n",
    "3. **Duplicate information** (age in years AND age in days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad2c3619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before manual selection:\n",
      "Features: ['bedrooms', 'size_sqft', 'age_years', 'location', 'price', 'house_id', 'random_number']\n",
      "\n",
      "After manual selection:\n",
      "Features: ['bedrooms', 'size_sqft', 'age_years', 'location', 'price']\n",
      "Removed 2 useless features\n"
     ]
    }
   ],
   "source": [
    "# Manual feature removal\n",
    "print(\"Before manual selection:\")\n",
    "print(f\"Features: {list(df.columns)}\")\n",
    "\n",
    "# Remove useless features\n",
    "features_to_remove = ['house_id', 'random_number']\n",
    "df_clean = df.drop(columns=features_to_remove)\n",
    "\n",
    "print(\"\\nAfter manual selection:\")\n",
    "print(f\"Features: {list(df_clean.columns)}\")\n",
    "print(f\"Removed {len(features_to_remove)} useless features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980bac0b",
   "metadata": {},
   "source": [
    "**Simple Guidelines:**\n",
    "- Remove ID numbers\n",
    "- Remove random/meaningless data  \n",
    "- Remove duplicate information\n",
    "- Keep features that make business sense\n",
    "\n",
    "---\n",
    "\n",
    "## üîç STEP 4: Quantitative Techniques\n",
    "\n",
    "Now we use statistical methods to find the best features automatically.\n",
    "\n",
    "### A. Filter Methods (Statistical Tests)\n",
    "\n",
    "**What they do:** Test how well each feature relates to the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1f686f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance scores:\n",
      "bedrooms: 164.28\n",
      "size_sqft: 496.65\n",
      "age_years: 1.94\n",
      "\n",
      "Top 2 features selected: ['bedrooms', 'size_sqft']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "X = df_clean[['bedrooms', 'size_sqft', 'age_years']]\n",
    "y = df_clean['price']\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),      # handle NaNs if any\n",
    "    (\"kbest\",   SelectKBest(score_func=f_regression, k=2))\n",
    "])\n",
    "\n",
    "X_selected = pipe.fit_transform(X, y)\n",
    "\n",
    "# map support back to original columns\n",
    "support = pipe.named_steps[\"kbest\"].get_support()\n",
    "scores = pipe.named_steps[\"kbest\"].scores_\n",
    "\n",
    "print(\"Feature importance scores:\")\n",
    "for feat, s in zip(X.columns, scores):\n",
    "    print(f\"{feat}: {s:.2f}\")\n",
    "\n",
    "selected_features = X.columns[support]\n",
    "print(f\"\\nTop 2 features selected: {list(selected_features)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "46a2f826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 2 features selected: ['bedrooms', 'size_sqft']\n"
     ]
    }
   ],
   "source": [
    "y_cls = pd.qcut(df_clean['price'], q=3, labels=False)  # low, mid, high\n",
    "selector = SelectKBest(score_func=f_classif, k=2)\n",
    "X_selected = selector.fit_transform(X, y_cls)\n",
    "\n",
    "# Get selected feature names\n",
    "selected_features = X.columns[selector.get_support()]\n",
    "\n",
    "print(\"\\nTop 2 features selected:\", list(selected_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa44bb51",
   "metadata": {},
   "source": [
    "### B. Wrapper Methods (Model-Based)\n",
    "\n",
    "**What they do:** Test different combinations of features with actual models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef202dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapper method selected: ['size_sqft', 'age_years']\n"
     ]
    }
   ],
   "source": [
    "# Simple wrapper method example\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Use a simple model to rank features\n",
    "model = LogisticRegression(max_iter=9000)\n",
    "selector = RFE(model, n_features_to_select=2)\n",
    "\n",
    "# This will try different feature combinations\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "selected_features = X.columns[selector.support_]\n",
    "\n",
    "print(f\"Wrapper method selected: {list(selected_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e95328",
   "metadata": {},
   "source": [
    "### C. Embedded Methods (Built-in Selection)\n",
    "\n",
    "**What they do:** Some models automatically tell us which features are important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "04dd67bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest feature importance:\n",
      "bedrooms: 0.291\n",
      "size_sqft: 0.363\n",
      "age_years: 0.346\n",
      "\n",
      "Important features (>0.1): ['bedrooms', 'size_sqft', 'age_years']\n"
     ]
    }
   ],
   "source": [
    "# Random Forest automatically calculates feature importance\n",
    "rf = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Get feature importance scores\n",
    "importance_scores = rf.feature_importances_\n",
    "\n",
    "print(\"Random Forest feature importance:\")\n",
    "for feature, importance in zip(X.columns, importance_scores):\n",
    "    print(f\"{feature}: {importance:.3f}\")\n",
    "\n",
    "# Select features above threshold\n",
    "threshold = 0.1  # Keep features with >10% importance\n",
    "important_features = [feature for feature, importance in \n",
    "                     zip(X.columns, importance_scores) \n",
    "                     if importance > threshold]\n",
    "\n",
    "print(f\"\\nImportant features (>{threshold}): {important_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412e0bc0",
   "metadata": {},
   "source": [
    "**Summary of Methods:**\n",
    "- **Filter:** Fast statistical tests\n",
    "- **Wrapper:** Test with actual models (slower but better)\n",
    "- **Embedded:** Built into some models (like Random Forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0c13d9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîç STEP 5: Handling categorical features\n",
    "\n",
    "### Q5: What about non-numeric features like location?\n",
    "\n",
    "**Answer:** Convert them to numbers using dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25ce9f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original location column:\n",
      "['City' 'Suburb' 'Rural']\n",
      "\n",
      "After creating dummy variables:\n",
      "['bedrooms', 'size_sqft', 'age_years', 'price', 'location_City', 'location_Rural', 'location_Suburb']\n",
      "\n",
      "Sample of encoded data:\n",
      "   location_City  location_Rural  location_Suburb\n",
      "0           True           False            False\n",
      "1          False           False             True\n",
      "2           True           False            False\n",
      "3          False            True            False\n",
      "4           True           False            False\n"
     ]
    }
   ],
   "source": [
    "# Our location feature is text, not numbers\n",
    "print(\"Original location column:\")\n",
    "print(df_clean['location'].unique())\n",
    "\n",
    "# Convert to dummy variables (one-hot encoding)\n",
    "df_encoded = pd.get_dummies(df_clean, columns=['location'])\n",
    "print(\"\\nAfter creating dummy variables:\")\n",
    "print(df_encoded.columns.tolist())\n",
    "\n",
    "# Now each location becomes a separate 0/1 column\n",
    "print(\"\\nSample of encoded data:\")\n",
    "print(df_encoded[['location_City', 'location_Rural', 'location_Suburb']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8b2825",
   "metadata": {},
   "source": [
    "**Key Point:** Machine learning needs numbers, so we convert categories into 0/1 columns.\n",
    "\n",
    "---\n",
    "\n",
    "## üîç STEP 6: Using Pipelines\n",
    "\n",
    "### Q6: How do we combine preprocessing and feature selection?\n",
    "\n",
    "**Answer:** Use pipelines to chain steps together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4492548f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline R¬≤ score: 0.772\n",
      "Pipeline selected features: ['bedrooms', 'size_sqft', 'age_years']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Create a regression pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),                    # Step 1: Scale the data\n",
    "    ('selector', SelectKBest(score_func=f_regression, k=3)),  # Step 2: Select best 3 features for regression\n",
    "    ('model', LinearRegression())                    # Step 3: Train regression model\n",
    "])\n",
    "\n",
    "# Prepare features\n",
    "X = df_encoded[['bedrooms', 'size_sqft', 'age_years']]\n",
    "y = df_encoded['price']  # Keep price as continuous\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Test the pipeline (returns R¬≤ score for regression)\n",
    "score = pipeline.score(X_test, y_test)\n",
    "print(f\"Pipeline R¬≤ score: {score:.3f}\")\n",
    "\n",
    "# See which features were selected\n",
    "selected_features = X.columns[pipeline.named_steps['selector'].get_support()]\n",
    "print(f\"Pipeline selected features: {list(selected_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a82f20",
   "metadata": {},
   "source": [
    "**Why use pipelines?**\n",
    "- Keeps everything organized\n",
    "- Prevents mistakes\n",
    "- Easy to reuse\n",
    "- Consistent preprocessing\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d948b43",
   "metadata": {},
   "source": [
    "### Key Takeaways:\n",
    "\n",
    "| What to Do | Why | How |\n",
    "|------------|-----|-----|\n",
    "| Remove IDs | They don't predict anything | `df.drop('customer_id')` |\n",
    "| Convert categories | ML needs numbers | `pd.get_dummies()` |\n",
    "| Test feature importance | Find what matters | `SelectKBest()` or `RandomForest` |\n",
    "| Use pipelines | Stay organized | `Pipeline([steps])` |\n",
    "| Start simple | Better to understand | Begin with few features |\n",
    "\n",
    "\n",
    "**Remember:** The goal is to find the features that help your model make better predictions. Start simple, remove the obvious bad features, then use the tools to find the best ones!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
